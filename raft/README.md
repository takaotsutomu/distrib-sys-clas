# Raft Distributed Consensus Protocol
[Raft](https://en.wikipedia.org/wiki/Raft_(algorithm)) distributed consensus protocol implementation without log compaction (snapshotting). The full implementation with log compaction is done and used in the **Fault-Tolerant Key/Value system** and **Fault-Tolerant Key/Value Store *Sharded*** projects. See the Raft library in either project for the full implementation. Either implementation reliably passed all the tests several hundred times. 

Due to the complex nature of distributed consensus (e.g., failures and uncertainty must be considered), translating the in-theory-simple Raft algorithm to a fully functioning implementation isn't trivial. Indeed, it took multiple tries and multiple mistakes to get it right and several important design decisions had to be made to make the implementation clean and efficient. For example, I considered using a pub-sub pattern with `map[chan bool]bool` (i.e., `map[voteCh]isOpen`) in `startLeaderElection`, which requires a separate lock around the code working with it and introduces extra complexity. But since the buffered channel not being referenced will be garbage-collected, the pub-sub pattern is not necessary. I tried using explicit timers for all events in Raft but later made my way to conditional variables and signals, which led to a much more elegant and faster implementation. 

```
Test (2A): initial election ...
  ... Passed --   3.1  3   60   16438    0
Test (2A): election after network failure ...
  ... Passed --   5.0  3  138   26962    0
Test (2A): multiple elections ...
  ... Passed --   5.6  7  636  126350    0
Test (2B): basic agreement ...
  ... Passed --   0.7  3   16    4406    3
Test (2B): RPC byte count ...
  ... Passed --   1.4  3   48  113986   11
Test (2B): agreement after follower reconnects ...
  ... Passed --   5.5  3  134   35831    8
Test (2B): no agreement if too many followers disconnect ...
  ... Passed --   3.5  5  212   43947    4
Test (2B): concurrent Start()s ...
  ... Passed --   0.6  3   20    5942    6
Test (2B): rejoin of partitioned leader ...
  ... Passed --   4.1  3  146   34316    4
Test (2B): leader backs up quickly over incorrect follower logs ...
  ... Passed --  16.9  5 2188 1853229  103
Test (2B): RPC counts aren't too high ...
  ... Passed --   2.1  3   62   19334   12
Test (2C): basic persistence ...
  ... Passed --   3.4  3   88   22065    6
Test (2C): more persistence ...
  ... Passed --  15.2  5 1000  207910   16
Test (2C): partitioned leader and one follower crash, leader restarts ...
  ... Passed --   1.8  3   40    9708    4
Test (2C): Figure 8 ...
  ... Passed --  31.8  5 1372  300910   65
Test (2C): unreliable agreement ...
  ... Passed --   3.8  5 1132  392474  246
Test (2C): Figure 8 (unreliable) ...
  ... Passed --  34.2  5 11280 24878162  197
Test (2C): churn ...
  ... Passed --  16.4  5 12016 41962013 2824
Test (2C): unreliable churn ...
  ... Passed --  16.2  5 2796 1473020  532
Test (2D): snapshots basic ...
  ... Passed --   3.8  3  522  187028  224
Test (2D): install snapshots (disconnect) ...
  ... Passed --  37.5  3 1517  886281  313
Test (2D): install snapshots (disconnect+unreliable) ...
  ... Passed --  71.5  3 2408 1386068  334
Test (2D): install snapshots (crash) ...
  ... Passed --  27.2  3 1193  705686  296
Test (2D): install snapshots (unreliable+crash) ...
  ... Passed --  38.6  3 1449  957220  307
Test (2D): crash and restart all servers ...
  ... Passed --   9.4  3  356  104994   73
PASS
ok      lab4/raft     359.414s
```